REPORT ON WHAT EACH TEAM MATE DID.

## SE22UARI080

 1. **preprocessing:** the data
 2.  **tokenization:** of the data
 3. **training:** with several bert models like distill bert , multilinguial bert,indic bert but all these attempts are failure as it is overfitting because the words are comparitively less compared to no of sentences present
 4. **Fine tuning:** tried fine tuneing using indic bert 

5. **Using Small Dataset**
As the subset is too big i used a small dataset extracted from main dataset.
it has about 25000 sentenence but is has 19k+words as their are more words comparing to sentences. the loss is too big.
i have updloaded this notebook in the github.


