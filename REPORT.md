REPORT ON WHAT EACH TEAM MATE DID.

## SE22UARI080

 1. **PREPROCESSING:** the data
 2.  **TOKENIZATION:** of the data
 3. **TRAINING:** with several bert models like distill bert , multilinguial bert,indic bert but all these attempts are failure as it is overfitting because the words are comparitively less compared to no of sentences present
 4. **FINE TUNING:** tried fine tuneing using indic bert 

5. **USING SMALL DATASET:** using INDIC BERT model
As the subset is too big i used a small dataset
extracted from main dataset.
it has about 25000 sentenence but is has 19k+words as their are
more words comparing to sentences. the loss is too big.
i have uploaded this notebook in the github.


