{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmZuwOng3FkJhnH4n7989K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshithamadarapu/Team16_Hinglish-Auto-suggestions/blob/main/DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv('/content/preprocessed_train.csv')\n",
        "val_df = pd.read_csv('/content/prerocessed_validation (1).csv')\n",
        "\n",
        "# Extract the `phrases` column\n",
        "train_sentences = train_df['phrases'].tolist()\n",
        "val_sentences = val_df['phrases'].tolist()"
      ],
      "metadata": {
        "id": "-WM0ORFr_sys"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer\n",
        "import random\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "def mask_text(sentences, tokenizer, mask_probability=0.15):\n",
        "    inputs, labels = [], []\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence\n",
        "        tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        # Create labels and randomly mask tokens\n",
        "        input_ids = tokenized[\"input_ids\"][0]\n",
        "        label_ids = input_ids.clone()\n",
        "\n",
        "        for i in range(len(input_ids)):\n",
        "            if random.random() < mask_probability and input_ids[i] != tokenizer.pad_token_id:\n",
        "                input_ids[i] = tokenizer.mask_token_id  # Replace token with [MASK]\n",
        "            else:\n",
        "                label_ids[i] = -100  # Ignore token for loss calculation\n",
        "\n",
        "        inputs.append(input_ids)\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    return {\"input_ids\": inputs, \"labels\": labels}\n",
        "\n",
        "# Prepare datasets\n",
        "train_data = mask_text(train_sentences, tokenizer)\n",
        "val_data = mask_text(val_sentences, tokenizer)"
      ],
      "metadata": {
        "id": "wQQQsFFZ_swV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class HinglishDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = HinglishDataset(train_data[\"input_ids\"], train_data[\"labels\"])\n",
        "val_dataset = HinglishDataset(val_data[\"input_ids\"], val_data[\"labels\"])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "_u-G2irI_stv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForMaskedLM, AdamW\n",
        "from transformers import get_scheduler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model\n",
        "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 3  # Assume 3 epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtI3mWlL_srI",
        "outputId": "2a63b273-a338-48c0-ee24-46088e2d2572"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForMaskedLM(\n",
              "  (activation): GELUActivation()\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (vocab_projector): Linear(in_features=768, out_features=119547, bias=True)\n",
              "  (mlm_loss_fct): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch {epoch}\")\n",
        "        loop.set_postfix(loss=loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcylktEG_snR",
        "outputId": "80d04a71-0165-49be-a552-165e067c0fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/11783 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Epoch 0:  93%|█████████▎| 10974/11783 [1:30:17<06:38,  2.03it/s, loss=1.14]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./hinglish_next_word_model')\n",
        "tokenizer.save_pretrained('./hinglish_next_word_model')"
      ],
      "metadata": {
        "id": "-GW0q9dXAYCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fill_mask = pipeline('fill-mask', model='./hinglish_next_word_model', tokenizer='./hinglish_next_word_model')\n",
        "\n",
        "# Test with an example\n",
        "text = \"Mujhe lagta hai ki [MASK]\"\n",
        "predictions = fill_mask(text)\n",
        "\n",
        "for pred in predictions:\n",
        "    print(f\"Suggested word: {pred['token_str']} with score: {pred['score']}\")"
      ],
      "metadata": {
        "id": "13dpj5sSAX8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "fill_mask = pipeline('fill-mask', model='./hinglish_next_word_model', tokenizer='./hinglish_next_word_model')\n",
        "\n",
        "def predict_next_word():\n",
        "    print(\"Hinglish Next-Word Prediction\")\n",
        "    print(\"Type a word or sentence to predict the next word.\")\n",
        "    print(\"Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a word or sentence: \").strip()\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Exiting. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Add the [MASK] token at the end of the input\n",
        "        input_with_mask = f\"{user_input} [MASK]\"\n",
        "\n",
        "        # Get predictions from the model\n",
        "        try:\n",
        "            predictions = fill_mask(input_with_mask)\n",
        "            print(\"\\nPredicted next words:\")\n",
        "            for i, pred in enumerate(predictions):\n",
        "                print(f\"{i+1}. {pred['token_str']} (score: {pred['score']:.4f})\")\n",
        "        except Exception as e:\n",
        "            print(\"Error during prediction:\", e)\n",
        "        print()\n",
        "\n",
        "# Run the prediction function\n",
        "predict_next_word()"
      ],
      "metadata": {
        "id": "gIoHshAeAX3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
