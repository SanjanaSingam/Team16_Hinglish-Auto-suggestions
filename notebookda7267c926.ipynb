{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10155842,"sourceType":"datasetVersion","datasetId":6270320}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\n# Paths\ntrain_path = \"/kaggle/input/nlp-autosuggestion/preprocessed_train.csv\"\nval_path = \"/kaggle/input/nlp-autosuggestion/prerocessed_validation.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:26:01.723004Z","iopub.execute_input":"2024-12-10T07:26:01.723246Z","iopub.status.idle":"2024-12-10T07:26:19.564570Z","shell.execute_reply.started":"2024-12-10T07:26:01.723220Z","shell.execute_reply":"2024-12-10T07:26:19.563879Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class HinglishDataset(Dataset):\n    def __init__(self, file_path, tokenizer, max_len=128, sample_size=10000):\n        self.data = pd.read_csv(file_path).sample(sample_size)  # Take a subset of the data\n        self.phrases = self.data['phrases'].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.phrases)\n\n    def __getitem__(self, index):\n        text = self.phrases[index]\n        encoded = self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        return encoded.input_ids.squeeze(0), encoded.attention_mask.squeeze(0)\n\n# Initialize Tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\n# Prepare DataLoaders\ntrain_dataset = HinglishDataset(train_path, tokenizer, sample_size=2000)  # Reduced size\nval_dataset = HinglishDataset(val_path, tokenizer, sample_size=500)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:26:19.566075Z","iopub.execute_input":"2024-12-10T07:26:19.566562Z","iopub.status.idle":"2024-12-10T07:26:20.882454Z","shell.execute_reply.started":"2024-12-10T07:26:19.566532Z","shell.execute_reply":"2024-12-10T07:26:20.881639Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00729272fef8475aa1920670bab74fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc767697383a4202a9180448217f178d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ade4df41eb2462d8f9f24a98d391400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"337ff3d1a0ae4898b3991603fbf32088"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Define Model\nclass LSTMWithDistilBERT(nn.Module):\n    def __init__(self, bert_model, hidden_size=128, num_layers=1, dropout=0.1):\n        super(LSTMWithDistilBERT, self).__init__()\n        self.bert = bert_model\n        for param in self.bert.parameters():\n            param.requires_grad = False  # Freeze BERT layers\n        self.lstm = nn.LSTM(\n            input_size=self.bert.config.hidden_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout\n        )\n        self.fc = nn.Linear(hidden_size, self.bert.config.vocab_size)\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        lstm_output, _ = self.lstm(bert_output)\n        output = self.fc(lstm_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:26:29.831792Z","iopub.execute_input":"2024-12-10T07:26:29.832130Z","iopub.status.idle":"2024-12-10T07:26:29.838900Z","shell.execute_reply.started":"2024-12-10T07:26:29.832102Z","shell.execute_reply":"2024-12-10T07:26:29.837918Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Initialize Model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbert_model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\nmodel = LSTMWithDistilBERT(bert_model).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:26:42.425314Z","iopub.execute_input":"2024-12-10T07:26:42.425935Z","iopub.status.idle":"2024-12-10T07:26:45.915596Z","shell.execute_reply.started":"2024-12-10T07:26:42.425897Z","shell.execute_reply":"2024-12-10T07:26:45.914646Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e2d47b0f59409c84b28d07b401dd29"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Training Setup\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=3e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:26:53.656152Z","iopub.execute_input":"2024-12-10T07:26:53.656902Z","iopub.status.idle":"2024-12-10T07:26:53.661658Z","shell.execute_reply.started":"2024-12-10T07:26:53.656835Z","shell.execute_reply":"2024-12-10T07:26:53.660716Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Mixed Precision Training\nscaler = torch.cuda.amp.GradScaler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:27:06.360259Z","iopub.execute_input":"2024-12-10T07:27:06.360916Z","iopub.status.idle":"2024-12-10T07:27:06.365457Z","shell.execute_reply.started":"2024-12-10T07:27:06.360875Z","shell.execute_reply":"2024-12-10T07:27:06.364478Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2145773025.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Training Loop\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scaler, epochs=3):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        loop = tqdm(train_loader, leave=True)\n        for input_ids, attention_mask in loop:\n            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs.view(-1, outputs.size(-1)), input_ids.view(-1))\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n            loop.set_description(f'Epoch {epoch + 1}')\n            loop.set_postfix(loss=loss.item())\n\n    # Validation\n    model.eval()\n    with torch.no_grad():\n        val_loss = 0\n        for input_ids, attention_mask in val_loader:\n            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs.view(-1, outputs.size(-1)), input_ids.view(-1))\n            val_loss += loss.item()\n        print(f'Validation Loss: {val_loss / len(val_loader):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:27:19.057477Z","iopub.execute_input":"2024-12-10T07:27:19.057991Z","iopub.status.idle":"2024-12-10T07:27:19.067880Z","shell.execute_reply.started":"2024-12-10T07:27:19.057932Z","shell.execute_reply":"2024-12-10T07:27:19.066370Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, criterion, optimizer, scaler, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:29:02.854423Z","iopub.execute_input":"2024-12-10T07:29:02.855252Z","iopub.status.idle":"2024-12-10T07:32:45.553300Z","shell.execute_reply.started":"2024-12-10T07:29:02.855215Z","shell.execute_reply":"2024-12-10T07:32:45.552335Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_23/947880128.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=3.84]\nEpoch 2: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=3.23]\nEpoch 3: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=2.65]\nEpoch 4: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=2.47]\nEpoch 5: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=2.02]\nEpoch 6: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=1.85]\nEpoch 7: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=1.52]\nEpoch 8: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=1.58]\nEpoch 9: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=1.28]\nEpoch 10: 100%|██████████| 63/63 [00:11<00:00,  5.71it/s, loss=1.33]\nEpoch 11: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=1.19]\nEpoch 12: 100%|██████████| 63/63 [00:11<00:00,  5.72it/s, loss=1.36]\nEpoch 13: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=1.16]\nEpoch 14: 100%|██████████| 63/63 [00:11<00:00,  5.71it/s, loss=1.63]\nEpoch 15: 100%|██████████| 63/63 [00:11<00:00,  5.70it/s, loss=1.06]\nEpoch 16: 100%|██████████| 63/63 [00:11<00:00,  5.69it/s, loss=1.03]\nEpoch 17: 100%|██████████| 63/63 [00:11<00:00,  5.71it/s, loss=1.08] \nEpoch 18: 100%|██████████| 63/63 [00:11<00:00,  5.71it/s, loss=1.04] \nEpoch 19: 100%|██████████| 63/63 [00:11<00:00,  5.72it/s, loss=1.01] \nEpoch 20: 100%|██████████| 63/63 [00:11<00:00,  5.73it/s, loss=0.975]\n/tmp/ipykernel_23/947880128.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 1.7605\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\n\n# Evaluation Function\ndef evaluate_model(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0\n    total_batches = 0\n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(val_loader, desc=\"Evaluating\"):\n            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs.view(-1, outputs.size(-1)), input_ids.view(-1))\n            val_loss += loss.item()\n            total_batches += 1\n    avg_loss = val_loss / total_batches\n    perplexity = torch.exp(torch.tensor(avg_loss))\n    print(f\"Validation Loss: {avg_loss:.4f}\")\n    print(f\"Perplexity: {perplexity:.4f}\")\n    return avg_loss, perplexity\n\n# Save Model\ndef save_model(model, tokenizer, model_path=\"lstm_distilbert_model.pt\", tokenizer_path=\"tokenizer\"):\n    torch.save(model.state_dict(), model_path)\n    tokenizer.save_pretrained(tokenizer_path)\n    print(f\"Model saved to {model_path}\")\n    print(f\"Tokenizer saved to {tokenizer_path}\")\n\n# Run Evaluation and Save Model\nvalidation_loss, validation_perplexity = evaluate_model(model, val_loader, criterion)\nsave_model(model, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:33:12.313552Z","iopub.execute_input":"2024-12-10T07:33:12.313930Z","iopub.status.idle":"2024-12-10T07:33:15.233276Z","shell.execute_reply.started":"2024-12-10T07:33:12.313899Z","shell.execute_reply":"2024-12-10T07:33:15.232411Z"}},"outputs":[{"name":"stderr","text":"Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipykernel_23/2590369786.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEvaluating: 100%|██████████| 16/16 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 1.7605\nPerplexity: 5.8154\nModel saved to lstm_distilbert_model.pt\nTokenizer saved to tokenizer\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Load Tokenizer\nfrom transformers import DistilBertTokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"tokenizer\")\n\n# Load Model\nmodel = LSTMWithDistilBERT(bert_model).to(device)\nmodel.load_state_dict(torch.load(\"lstm_distilbert_model.pt\"))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:33:43.999637Z","iopub.execute_input":"2024-12-10T07:33:44.000006Z","iopub.status.idle":"2024-12-10T07:33:44.953051Z","shell.execute_reply.started":"2024-12-10T07:33:43.999974Z","shell.execute_reply":"2024-12-10T07:33:44.952199Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/444943344.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"lstm_distilbert_model.pt\"))\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"LSTMWithDistilBERT(\n  (bert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): DistilBertSdpaAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lstm): LSTM(768, 128, batch_first=True, dropout=0.1)\n  (fc): Linear(in_features=128, out_features=119547, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=1.0):\n    \"\"\"\n    Predict the next word(s) using sampling for variability.\n\n    Parameters:\n    - model: Trained model\n    - tokenizer: Tokenizer for input/output\n    - input_text: Starting text\n    - max_length: Number of tokens to predict\n    - temperature: Controls randomness in predictions; lower = deterministic\n\n    Returns:\n    - Generated text\n    \"\"\"\n    model.eval()\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    attention_mask = torch.ones(input_ids.shape).to(device)\n\n    predicted_text = input_text\n    with torch.no_grad():\n        for _ in range(max_length):\n            with torch.amp.autocast(device_type=\"cuda\"):  # Updated for warning\n                outputs = model(input_ids, attention_mask)\n\n            logits = outputs[:, -1, :] / temperature  # Apply temperature scaling\n            probabilities = F.softmax(logits, dim=-1)\n\n            # Use torch.multinomial for sampling\n            predicted_token_id = torch.multinomial(probabilities, num_samples=1).squeeze(1)\n\n            # Decode and append\n            predicted_word = tokenizer.decode(predicted_token_id.item())  # Extract single token ID\n            if predicted_word in tokenizer.all_special_tokens:\n                break\n            predicted_text += \" \" + predicted_word.strip()\n\n            # Update input_ids for next prediction\n            input_ids = torch.cat([input_ids, predicted_token_id.unsqueeze(0)], dim=1)\n            attention_mask = torch.cat(\n                [attention_mask, torch.ones((1, 1)).to(device)], dim=1\n            )\n\n    return predicted_text.strip()\n\n# Example usage\ninput_text = \"u\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:46:15.428090Z","iopub.execute_input":"2024-12-10T07:46:15.428943Z","iopub.status.idle":"2024-12-10T07:46:15.459534Z","shell.execute_reply.started":"2024-12-10T07:46:15.428891Z","shell.execute_reply":"2024-12-10T07:46:15.458518Z"}},"outputs":[{"name":"stdout","text":"Input Text: u\nPredicted Text: u ky\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"input_text = \"haa\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:47:24.680264Z","iopub.execute_input":"2024-12-10T07:47:24.680985Z","iopub.status.idle":"2024-12-10T07:47:24.705697Z","shell.execute_reply.started":"2024-12-10T07:47:24.680948Z","shell.execute_reply":"2024-12-10T07:47:24.704446Z"}},"outputs":[{"name":"stdout","text":"Input Text: haa\nPredicted Text: haa хорошо\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"input_text = \"par\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:49:11.903190Z","iopub.execute_input":"2024-12-10T07:49:11.903543Z","iopub.status.idle":"2024-12-10T07:49:11.925409Z","shell.execute_reply.started":"2024-12-10T07:49:11.903516Z","shell.execute_reply":"2024-12-10T07:49:11.924585Z"}},"outputs":[{"name":"stdout","text":"Input Text: par\nPredicted Text: par ba\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"input_text = \"ki\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:50:28.985677Z","iopub.execute_input":"2024-12-10T07:50:28.986542Z","iopub.status.idle":"2024-12-10T07:50:29.009694Z","shell.execute_reply.started":"2024-12-10T07:50:28.986506Z","shell.execute_reply":"2024-12-10T07:50:29.008147Z"}},"outputs":[{"name":"stdout","text":"Input Text: ki\nPredicted Text: ki previous\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"input_text = \"baje\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:52:41.078734Z","iopub.execute_input":"2024-12-10T07:52:41.079557Z","iopub.status.idle":"2024-12-10T07:52:41.100873Z","shell.execute_reply.started":"2024-12-10T07:52:41.079522Z","shell.execute_reply":"2024-12-10T07:52:41.100069Z"}},"outputs":[{"name":"stdout","text":"Input Text: baje\nPredicted Text: baje районов\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"input_text = \"raha\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:53:08.121675Z","iopub.execute_input":"2024-12-10T07:53:08.122510Z","iopub.status.idle":"2024-12-10T07:53:08.135778Z","shell.execute_reply.started":"2024-12-10T07:53:08.122471Z","shell.execute_reply":"2024-12-10T07:53:08.134989Z"}},"outputs":[{"name":"stdout","text":"Input Text: raha\nPredicted Text: raha\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"input_text = \"yaad\"\npredicted_text = predict_next_word_with_sampling(model, tokenizer, input_text, max_length=5, temperature=0.8)\nprint(f\"Input Text: {input_text}\")\nprint(f\"Predicted Text: {predicted_text}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:53:34.883659Z","iopub.execute_input":"2024-12-10T07:53:34.884280Z","iopub.status.idle":"2024-12-10T07:53:34.898467Z","shell.execute_reply.started":"2024-12-10T07:53:34.884242Z","shell.execute_reply":"2024-12-10T07:53:34.897603Z"}},"outputs":[{"name":"stdout","text":"Input Text: yaad\nPredicted Text: yaad\n","output_type":"stream"}],"execution_count":63}]}